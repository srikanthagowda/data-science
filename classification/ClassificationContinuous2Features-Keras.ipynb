{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification from 2 features with Keras\n",
    "\n",
    "Classification using Keras, performing a classification similar to ClassificationContinuous2Features-TensorFlow. \n",
    "\n",
    "Based on the same data model as in the ClassificationContinuous2Features workbook.\n",
    "\n",
    "The classification is on a single boundary defined by a continuous function and added white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as pltcolors\n",
    "from sklearn import metrics as skMetrics\n",
    "import scikitplot as skplt\n",
    "import scipy as sy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usingTensorBoard = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Quadratic function a boundary between positive and negative values\n",
    "\n",
    "Adding some unknown as a Gaussian noise\n",
    "\n",
    "The values of X are uniformly distributed and independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two features, Gaussian noise\n",
    "nFeatures = 2\n",
    "def generateBatch(N):\n",
    "    #\n",
    "    xMin = 0\n",
    "    xMax = 1\n",
    "    b = 0.1\n",
    "    std = 0.1\n",
    "    #\n",
    "    x = random.uniform(xMin, xMax, (N, 2))\n",
    "    # 4th degree relation to shape the boundary\n",
    "    boundary = 2*(x[:,0]**4 + (x[:,0]-0.3)**3 + b)\n",
    "    # Adding some gaussian noise\n",
    "    labels = boundary + random.normal(0, std, N) > x[:,1]\n",
    "    return (x, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 2000\n",
    "# x has 1 dim in R, label has 1 dim in B\n",
    "xTrain, labelTrain = generateBatch(N)\n",
    "\n",
    "colors = ['blue','red']\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(xTrain[:,0], xTrain[:,1], c=labelTrain, cmap=pltcolors.ListedColormap(colors), marker=',', alpha=0.2)\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.title('Generated train data')\n",
    "plt.grid()\n",
    "cb = plt.colorbar()\n",
    "loc = np.arange(0,1,1./len(colors))\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels([0,1])\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(xTrain[:,0], labelTrain, marker=',', alpha=0.01)\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('label')\n",
    "plt.grid()\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(xTrain[:,1], labelTrain, marker=',', alpha=0.01)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('label')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count, bins, ignored = plt.hist(labelTrain, 10, density=True, alpha=0.5)\n",
    "p = np.mean(labelTrain)\n",
    "print('Bernouilli parameter of the distribution:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data for verification of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest, labelTest = generateBatch(N)\n",
    "testColors = ['navy', 'orangered']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorFlow's Keras\n",
    "\n",
    "## Logistic regression using Keras\n",
    "\n",
    "References:\n",
    "- https://www.tensorflow.org/tutorials/keras/basic_classification\n",
    "- https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "nEpoch = 200\n",
    "nBatch = 32 # 32 is default\n",
    "model = keras.models.Sequential([\n",
    "  keras.layers.Dense(1, activation=tf.sigmoid, input_shape=[nFeatures])\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy', #'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "callbacks = []\n",
    "if usingTensorBoard:\n",
    "    ks = keras.callbacks.TensorBoard(log_dir=\"./logs/\", \n",
    "                                     histogram_freq=1, write_graph=True, write_grads=True, batch_size=1)\n",
    "    callbacks = [ks]\n",
    "\n",
    "hist = model.fit(xTrain, labelTrain, epochs=nEpoch, batch_size=nBatch, verbose=0, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.get_weights()\n",
    "print('Est W=', weights.reshape(-1), ', b=', biases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.grid()\n",
    "plt.title('Loss')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.grid()\n",
    "plt.title('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(xTest, labelTest)\n",
    "yEst = model.predict(xTest).reshape(-1)\n",
    "labelEst = yEst > 0.5\n",
    "#labelEst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(xTest[:,0], xTest[:,1], c=labelEst, cmap=pltcolors.ListedColormap(testColors), marker='x', alpha=0.2);\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.grid()\n",
    "plt.title('Estimated')\n",
    "cb = plt.colorbar()\n",
    "loc = np.arange(0,1,1./len(testColors))\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels([0,1]);\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(yEst, 10, density=True, alpha=0.5)\n",
    "plt.title('Bernouilli parameter =' + str(np.mean(labelEst)))\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(xTest[:,0], xTest[:,1], c=labelTest, cmap=pltcolors.ListedColormap(colors), marker='x', alpha=0.2);\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.grid()\n",
    "plt.title('Generator')\n",
    "cb = plt.colorbar()\n",
    "loc = np.arange(0,1,1./len(colors))\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels([0,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(labelTest, labelEst, normalize=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skMetrics.classification_report(labelTest, labelEst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization done using TensorBoard\n",
    "\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a regularizer\n",
    "\n",
    "We have seen in ClassificationContinuous2Features-TensorFlow that the solution is not unique. Let's add a constraint through a regularizer.\n",
    "\n",
    "Reference: https://machinelearningmastery.com/how-to-reduce-generalization-error-in-deep-neural-networks-with-activity-regularization-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.Sequential([\n",
    "  keras.layers.Dense(1, activation='linear', input_shape=[nFeatures],\n",
    "                        bias_regularizer=keras.regularizers.l1(0.01),      # <---\n",
    "                        activity_regularizer=keras.regularizers.l1(0.01)), # <----\n",
    "  keras.layers.Activation(tf.sigmoid)                                      # <----\n",
    "])\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = []\n",
    "if usingTensorBoard:\n",
    "    ks = keras.callbacks.TensorBoard(log_dir=\"./logs2/\", \n",
    "                                     histogram_freq=1, write_graph=True, write_grads=True, batch_size=1)\n",
    "    callbacks = [ks]\n",
    "\n",
    "hist2 = model2.fit(xTrain, labelTrain, epochs=nEpoch, batch_size=nBatch, verbose=0, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(hist2.history['loss'])\n",
    "plt.grid()\n",
    "plt.title('Loss')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(hist2.history['acc'])\n",
    "plt.grid()\n",
    "plt.title('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2, biases2 = model2.get_weights()\n",
    "print('Regularizer W=', weights2.reshape(-1), ', b=', biases2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a kernel regularizer, the convergence seems faster, it depends however on the initialization values.\n",
    "\n",
    "The weights and bias are not that different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model with regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yEst2 = model2.predict(xTest).reshape(-1)\n",
    "labelEst2 = yEst2 > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(xTest[:,0], xTest[:,1], c=labelEst2, cmap=pltcolors.ListedColormap(testColors), marker='x', alpha=0.2);\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.grid()\n",
    "plt.title('Estimated with regularizer')\n",
    "cb = plt.colorbar()\n",
    "loc = np.arange(0,1,1./len(testColors))\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels([0,1]);\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(yEst2, 10, density=True, alpha=0.5)\n",
    "plt.title('Bernouilli parameter =' + str(np.mean(labelEst2)));\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(xTest[:,0], xTest[:,1], c=labelTest, cmap=pltcolors.ListedColormap(colors), marker='x', alpha=0.2);\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.grid()\n",
    "plt.title('Generator')\n",
    "cb = plt.colorbar()\n",
    "loc = np.arange(0,1,1./len(colors))\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels([0,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(labelTest, labelEst2, normalize=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding capacity to the model\n",
    "\n",
    "The above model is not able to match the actual model boundary as its capacity is a simple linear, 1st degree, separation of the plan\n",
    "\n",
    "Let's add more neurons or more layers to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two layers (4 -> 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpoch3 = 400\n",
    "model3 = keras.models.Sequential([\n",
    "  keras.layers.Dense(4, activation='linear', input_shape=[nFeatures], # <---\n",
    "                    bias_regularizer=keras.regularizers.l2(0.01),     \n",
    "                    activity_regularizer=keras.regularizers.l2(0.01)),\n",
    "  keras.layers.Activation(tf.nn.relu),\n",
    "  #keras.layers.Dropout(0.01),\n",
    "  keras.layers.Dense(1, activation=tf.sigmoid) # <---\n",
    "])\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy', #'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "callbacks = []\n",
    "if usingTensorBoard:\n",
    "    ks = keras.callbacks.TensorBoard(log_dir=\"./logs3/\", histogram_freq=1, write_graph=True, write_grads=True, batch_size=1)\n",
    "    callbacks = [ks]\n",
    "    \n",
    "hist3 = model3.fit(xTrain, labelTrain, epochs=nEpoch3, batch_size=nBatch, verbose=0, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(hist3.history['loss'])\n",
    "plt.grid()\n",
    "plt.title('Loss')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(hist3.history['acc'])\n",
    "plt.grid()\n",
    "plt.title('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights3_1, biases3_1, weights3_2, biases3_2 = model3.get_weights()\n",
    "print('2 Layers')\n",
    "print('W1 =', weights3_1.reshape(-1))\n",
    "print('b1 =', biases3_1)\n",
    "print('W2 =', weights3_2.reshape(-1))\n",
    "print('b2 =', biases3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model with two layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yEst3 = model3.predict(xTest).reshape(-1)\n",
    "labelEst3 = yEst3 > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(xTest[:,0], xTest[:,1], c=labelEst3, cmap=pltcolors.ListedColormap(testColors), marker='x', alpha=0.2);\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.grid()\n",
    "plt.title('Estimated with 2 layers')\n",
    "cb = plt.colorbar()\n",
    "loc = np.arange(0,1,1./len(testColors))\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels([0,1]);\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(yEst3, 10, density=True, alpha=0.5)\n",
    "plt.title('Bernouilli parameter =' + str(np.mean(labelEst3)));\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(xTest[:,0], xTest[:,1], c=labelTest, cmap=pltcolors.ListedColormap(colors), marker='x', alpha=0.2);\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.grid()\n",
    "plt.title('Generator')\n",
    "cb = plt.colorbar()\n",
    "loc = np.arange(0,1,1./len(colors))\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels([0,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(labelTest, labelEst3, normalize=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skMetrics.classification_report(labelTest, labelEst3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The network if able to match much better the generation function leading to an improvement in the accuracy from 90% to ~94%, given a gaussian noise of std=0.1.\n",
    "\n",
    "On the accuracy plot, we see that the optimization happens in two steps: initial convergence to 90% accuracy (as the simpler network), then slower optimization from 90 to 94% of accuracy.\n",
    "\n",
    "However, the experiments have shown much more instability of the optimizer with more layers, even sometimes failing to converge. With this network, the regularizer seems mandatory otherwise the second round of optimization is never reached."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
